{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe1ac71",
   "metadata": {},
   "source": [
    "# Import Relevant Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc505de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw, openpyxl, pandas as pd, re, requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41717c23-5dde-47ed-be7c-972741900c99",
   "metadata": {},
   "source": [
    "# Initializing praw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946bbc7b-934c-4eba-8deb-5355b3670d18",
   "metadata": {},
   "source": [
    "> Fields to be filled in: `client_id, client_secret, password, user_agent, username`\n",
    "> \n",
    "> More details can be found at https://praw.readthedocs.io/en/stable/getting_started/quick_start.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3837cb5-fae4-40ee-93d0-3dbb52957d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"client_id\",\n",
    "    client_secret=\"client_secret\",\n",
    "    password=\"password\",\n",
    "    user_agent=\"user_agent\",\n",
    "    username=\"username\",\n",
    ") \n",
    "print(reddit.user.me())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c4155f-e91f-49ee-9a16-87ecd84749a4",
   "metadata": {},
   "source": [
    "# Class to Extract Comments from Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384e3f21-2c49-49ac-8cba-ed26edbcfb37",
   "metadata": {},
   "source": [
    "> API Keys need to be filled in [`api_key`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26710e8-37ff-421f-9e50-fbbb9985d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "class polarity_classification:\n",
    "    wb = None\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.FILENAME = f\"{self.name}.xlsx\"\n",
    "        self.LANG = 'en' #Language of API to be used\n",
    "        self.APIKEY = ''\n",
    "        self.APIURL = ''\n",
    "\n",
    "    def open_or_create_workbook(self):\n",
    "        try:\n",
    "            self.wb = openpyxl.load_workbook(filename = self.FILENAME)\n",
    "            print('Opened workbook')\n",
    "        except:\n",
    "            self.wb = openpyxl.Workbook()\n",
    "            print('Create new workbook')\n",
    "            print(self.wb.sheetnames)\n",
    "            ws = self.wb.active\n",
    "            ws.title = f\"Unprocessed Comments\"\n",
    "            heading = [['Comment', 'Score', 'Author', 'Comment ID']]\n",
    "            for row in heading:\n",
    "                ws.append(row)\n",
    "            self.wb.save(self.FILENAME)\n",
    "            print(self.wb.path)\n",
    "\n",
    "    def close_workbook(self):\n",
    "        self.wb.save(self.FILENAME)\n",
    "        self.wb.close()\n",
    "\n",
    "    def get_api_ws(self, api_type):\n",
    "        if api_type == 'concept_parsing':\n",
    "            api_key = ''\n",
    "            ws_name = 'Concept Parsing'\n",
    "        elif api_type == 'subjectivity_detection':\n",
    "            api_key = ''\n",
    "            ws_name = 'Subjectivity Detection'\n",
    "        elif api_type == 'polarity_classification':\n",
    "            api_key = ''\n",
    "            ws_name = 'Polarity Classification'\n",
    "        elif api_type == 'intensity_ranking':\n",
    "            api_key = ''\n",
    "            ws_name = 'Intensity Ranking'\n",
    "        elif api_type == 'emotion_recognition':\n",
    "            api_key = ''\n",
    "            ws_name = 'Emotion Recognition'\n",
    "        elif api_type == 'aspect_extraction':\n",
    "            api_key = ''\n",
    "            ws_name = 'Aspect Extraction'\n",
    "        elif api_type == 'personality_prediction':\n",
    "            api_key = ''\n",
    "            ws_name = 'Personality Prediction'\n",
    "        elif api_type == 'sarcasm_identification':\n",
    "            api_key = ''\n",
    "            ws_name = 'Sarcasm Detection'\n",
    "        elif api_type == 'depression_categorization':\n",
    "            api_key = ''\n",
    "            ws_name = 'Depression Categorization'\n",
    "        elif api_type == 'toxicity_spotting':\n",
    "            api_key = ''\n",
    "            ws_name = 'Toxicity Spotting'\n",
    "        elif api_type == 'engagement_measurement':\n",
    "            api_key = ''\n",
    "            ws_name = 'Engagement Measurement'\n",
    "        elif api_type == 'wellbeing_assessment':\n",
    "            api_key = ''\n",
    "            ws_name = 'Well-Being Assessment'\n",
    "        else:\n",
    "            api_key = None\n",
    "            ws_name = None\n",
    "        self.APIKEY = api_key\n",
    "        self.APIURL = f'https://sentic.net/api/{self.LANG}/{self.APIKEY}.py?text='\n",
    "        ws = self.open_or_copy_worksheet(ws_name)\n",
    "        ws.cell(row= 1, column = 5, value = ws_name)\n",
    "\n",
    "        return self.open_or_copy_worksheet(ws_name), ws_name\n",
    "\n",
    "    def extract_comments(self, url):\n",
    "        ws_name = 'Unprocessed Comments'\n",
    "        \n",
    "        api_df = pd.read_excel(self.FILENAME, sheet_name=ws_name)\n",
    "        df = pd.DataFrame({'Comment': [], 'Score': [],'Author': [],'Comment ID': []})\n",
    "        submission = reddit.submission(url=url)\n",
    "        bot_list = ['AmputatorBot', 'XGramatik-Bot', 'RemindMeBot']\n",
    "        single_word, deleted_comments, removed_comments, duplicates, gif_count, url_count, total = 0, 0, 0, 0, 0, 0, 0\n",
    "        bot_comments = 0\n",
    "        comment_count, comment_after_deduplication = 0,0\n",
    "        \n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "            total+=1\n",
    "            commentor = comment.author\n",
    "            if commentor:\n",
    "                username = f'{commentor.name}'\n",
    "            else:\n",
    "                username = ''\n",
    "            if username in bot_list:\n",
    "                bot_comments += 1\n",
    "                continue\n",
    "            existing_comment = api_df['Comment ID'] == comment.id\n",
    "            if existing_comment.any():\n",
    "                continue\n",
    "                \n",
    "            text = comment.body\n",
    "\n",
    "            if text == '[deleted]':\n",
    "                deleted_comments += 1\n",
    "                continue\n",
    "            elif text == '[removed]':\n",
    "                removed_comments += 1\n",
    "                continue\n",
    "\n",
    "            count = len(re.findall(r'\\w+', text))\n",
    "            if count<=1:\n",
    "                single_word+=1\n",
    "                continue\n",
    "\n",
    "            gif_pattern = r\"!\\[gif\\]\\(giphy\\|[A-Za-z0-9]+(?:\\|[A-Za-z0-9]+)*\\)\"\n",
    "            gifs = re.findall(gif_pattern, text, flags=re.M)\n",
    "            url_pattern = url_pattern = r'https?://\\S+'\n",
    "            urls = re.findall(url_pattern, text, flags=re.M)\n",
    "            if urls:\n",
    "                text = re.sub(url_pattern, '', text, flags=re.M)\n",
    "                count = len(re.findall(r'\\w+', text))\n",
    "                if count<=1:\n",
    "                    single_word+=1\n",
    "                    continue\n",
    "                url_count+=1\n",
    "            if gifs:\n",
    "                text = re.sub(gif_pattern, '', text, flags=re.M)\n",
    "                count = len(re.findall(r'\\w+', text))\n",
    "                if count<=1:\n",
    "                    single_word+=1\n",
    "                    continue\n",
    "                gif_count+=1\n",
    "\n",
    "            text = re.sub(r\"[;&#{}]\", \":\", text)\n",
    "\n",
    "            new_row = pd.DataFrame({'Comment': [text], 'Score': [comment.score],'Author': [username],'Comment ID': [comment.id]})\n",
    "\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "            comment_count += 1\n",
    "        \n",
    "        df = df.drop_duplicates(subset=['Comment'], keep='first')\n",
    "        \n",
    "        with pd.ExcelWriter(self.FILENAME, engine='openpyxl', mode='a',if_sheet_exists='overlay') as writer:\n",
    "            book = writer.book\n",
    "            if ws_name in book.sheetnames:\n",
    "                sheet = book[ws_name]\n",
    "                startrow = sheet.max_row\n",
    "            else:\n",
    "                startrow = 0\n",
    "            df.to_excel(writer, index=False, sheet_name=ws_name, startrow=startrow, header=False if startrow > 0 else True)\n",
    "\n",
    "            \n",
    "        print(f'comment_count: {comment_count}')\n",
    "        print(f'comment_count without duplicates: {len(df.index)}')\n",
    "        print(f'bot_comments: {bot_comments}')     \n",
    "        print(f'duplicates: {duplicates}')\n",
    "        print(f'single_word: {single_word}')\n",
    "        print(f'deleted_comments: {deleted_comments}')\n",
    "        print(f'removed_comments: {removed_comments}')\n",
    "        print(f'gif_count: {gif_count}')\n",
    "        print(f'url_count: {url_count}')\n",
    "        print(f'total: {total}')\n",
    "\n",
    "    def run_api(self, api_type):\n",
    "        ws, ws_name = self.get_api_ws(api_type)\n",
    "        self.APIURL = f'https://sentic.net/api/en/{self.APIKEY}.py?text='\n",
    "        self.close_workbook()\n",
    "        api_df = pd.read_excel(self.FILENAME, sheet_name=ws_name)\n",
    "        \n",
    "        api_df = self.api_request(api_df)\n",
    "\n",
    "        if ws_name in self.wb.sheetnames:\n",
    "            sheet = self.wb[ws_name]\n",
    "            self.wb.remove(sheet)\n",
    "            self.close_workbook()\n",
    "        \n",
    "        with pd.ExcelWriter(self.FILENAME, engine='openpyxl', mode='a') as writer:\n",
    "            api_df.to_excel(writer, index=False, sheet_name=ws_name)\\\n",
    "                \n",
    "    def api_request(self, api_df):\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36'}\n",
    "        count = 1\n",
    "        total_rows = api_df.shape[0]\n",
    "        for index, row in api_df.iterrows():\n",
    "            clear_output(wait=True)\n",
    "            if pd.isna(api_df.iloc[index, 4]):\n",
    "                text = api_df.at[index, 'Comment']\n",
    "                # print(text)\n",
    "                label = str(requests.get(self.APIURL + str(text), headers=headers).content)[2:-3]\n",
    "                # print(label)\n",
    "                api_df.iloc[index, 4] = label\n",
    "                print(f'{count}/{total_rows} Comment ID: {api_df.at[index, \"Comment ID\"]} Label: {api_df.iloc[index, 4]}')\n",
    "            count+=1\n",
    "        return api_df\n",
    "    \n",
    "    def open_or_copy_worksheet(self, worksheet_name):\n",
    "        try:\n",
    "            if self.wb[worksheet_name]:\n",
    "                return self.wb[worksheet_name]\n",
    "        except:\n",
    "            api_ws = self.wb.copy_worksheet(self.wb[\"Unprocessed Comments\"])\n",
    "            api_ws.title = worksheet_name\n",
    "            return api_ws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59b3dc2-db76-4a70-ae30-b36461b722a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T01:00:56.097993Z",
     "iopub.status.busy": "2025-05-05T01:00:56.097993Z",
     "iopub.status.idle": "2025-05-05T01:00:56.114346Z",
     "shell.execute_reply": "2025-05-05T01:00:56.112833Z",
     "shell.execute_reply.started": "2025-05-05T01:00:56.097993Z"
    }
   },
   "source": [
    "# Creating Object and Extracting Reddit Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2788e034-5cd5-4b9b-8ffe-eca2a0488e36",
   "metadata": {},
   "source": [
    "> Fields to be filled in: `reddit_url, object_name, excel_workbook`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83802822-0121-4a1f-9803-00b4c7d8f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"reddit_url\"\n",
    "object_name = polarity_classification(\"excel_workbook\")\n",
    "object_name.open_or_create_workbook()\n",
    "object_name.extract_comments(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e463d92-819c-4ef5-9ab5-b64df60c74f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T01:00:56.097993Z",
     "iopub.status.busy": "2025-05-05T01:00:56.097993Z",
     "iopub.status.idle": "2025-05-05T01:00:56.114346Z",
     "shell.execute_reply": "2025-05-05T01:00:56.112833Z",
     "shell.execute_reply.started": "2025-05-05T01:00:56.097993Z"
    }
   },
   "source": [
    "# Running API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d441cd7-2c40-4b65-b906-7032b25c5752",
   "metadata": {},
   "source": [
    "> Fields to be filled in: `object_name, api_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c945519-4a37-4429-982f-e9444b2700d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_name.run_api('api_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1403d6-6124-43bb-ae6c-42ba3fc225cf",
   "metadata": {},
   "source": [
    "# End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
